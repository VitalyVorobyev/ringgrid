# Task: PERF-001: Establish Comprehensive Performance Tracing Baseline and Benchmark Harness

- **Type:** perf
- **Priority:** P1
- **Requesting role:** Project Lead
- **Assigned workflow:** performance-optimization

## Problem Statement

The project currently lacks a stable, repeatable performance-tracing baseline for core hot paths. We need a comprehensive setup that makes optimization decisions data-driven (not intuition-driven). This task is baseline-focused: establish deterministic benchmark/profiling infrastructure, capture objective hotspot evidence, and produce a prioritized optimization plan for follow-up PERF tasks.

## Affected Pipeline Stages

Which of the 13 stages are impacted? List by number and name.

1. [x] Proposal
2. [x] Outer Estimate
3. [x] Outer Fit
4. [ ] Decode
5. [ ] Inner Estimate
6. [ ] Dedup
7. [ ] Projective Center (1st pass)
8. [x] Global Filter
9. [ ] H-guided Refine
10. [ ] Projective Center (2nd pass)
11. [ ] Completion
12. [ ] Projective Center (3rd pass)
13. [ ] Final H Refit

## Affected Modules

List file paths under `crates/ringgrid/src/`:

- `crates/ringgrid/src/detector/proposal.rs`
- `crates/ringgrid/src/ring/radial_profile.rs`
- `crates/ringgrid/src/conic/fit.rs`
- `crates/ringgrid/src/conic/ransac.rs`
- `crates/ringgrid/src/homography/core.rs`
- `crates/ringgrid/src/pipeline/fit_decode.rs`
- `crates/ringgrid/src/pipeline/finalize.rs`

## Public API Impact

- [x] No API changes
- [ ] New public types (list them)
- [ ] Changed type signatures (list them)
- [ ] New config fields (list them, must have `Default`)
- [ ] New `Detector` methods (justify)

## Acceptance Criteria

- [ ] Criterion harness exists for core operations with deterministic inputs and stable names:
  - `proposal_1280x1024`
  - `proposal_1920x1080`
  - `ellipse_fit_50pts`
  - `radial_profile_32r_180a`
- [ ] Baseline performance report is produced and attached in `state/sessions/` (benchmark numbers + environment notes).
- [ ] At least one representative `detect()` flamegraph is captured and top-3 wall-time hotspots are documented.
- [ ] Allocation profile for `detect()` is documented in the baseline report.
- [ ] PERF follow-up plan is added to backlog with ordered optimization candidates based on measured hotspots.
- [ ] `cargo test --workspace --all-features` passes
- [ ] `cargo clippy --all-targets --all-features -- -D warnings` clean
- [ ] Validation requirements for follow-up optimization tasks are explicit: include blur=3.0 batch, `tools/run_reference_benchmark.sh`, and `tools/run_distortion_benchmark.sh`

## Accuracy Constraints

- Center error target: mean regression <= +0.01 px versus chosen baseline run for this task
- Decode success rate: no measurable drop beyond statistical noise on deterministic eval batch
- Homography reprojection: no material regression vs baseline (track mean delta; investigate if > +0.02 px)

## Performance Constraints

- Latency budget: this task establishes baseline only (no speedup target in PERF-001 itself)
- Allocation limits: no instrumentation-driven new per-candidate allocations in proposal/radial-profile/ellipse-fit loops

## Python Tooling Changes

- [x] No Python changes needed
- [ ] New scoring metric in `score_detect.py`
- [ ] New visualization in `viz_detect_debug.py`
- [ ] New synthetic generation option in `gen_synth.py`
- [ ] Other: [describe]

## Notes

- This task is intentionally baseline-only; optimization work starts in follow-up PERF tasks after measurement evidence is published.
- Start role is Performance Engineer per `workflows/performance-optimization.md`.
- If optimization requires mathematical tradeoffs (search strategy, model approximation, threshold semantics), hand off to Algorithm Engineer with quantified hotspot evidence.
