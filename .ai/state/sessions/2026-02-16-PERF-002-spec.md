# Task: PERF-002: Optimize Proposal Hotspot (`detector::proposal::find_proposals`)

- **Type:** perf
- **Priority:** P1
- **Requesting role:** Project Lead
- **Assigned workflow:** performance-optimization

## Problem Statement

PERF-001 baseline profiling identified `detector::proposal::find_proposals` as the dominant detect-time hotspot (~61.11% wall-time share on representative run). We need targeted optimization of proposal-stage hot loops (gradient/voting/NMS) with measurable speedup while preserving detection accuracy on challenging validation suites.

## Affected Pipeline Stages

Which of the 13 stages are impacted? List by number and name.

1. [x] Proposal
2. [ ] Outer Estimate
3. [ ] Outer Fit
4. [ ] Decode
5. [ ] Inner Estimate
6. [ ] Dedup
7. [ ] Projective Center (1st pass)
8. [ ] Global Filter
9. [ ] H-guided Refine
10. [ ] Projective Center (2nd pass)
11. [ ] Completion
12. [ ] Projective Center (3rd pass)
13. [ ] Final H Refit

## Affected Modules

List file paths under `crates/ringgrid/src/`:

- `crates/ringgrid/src/detector/proposal.rs`
- `crates/ringgrid/src/pipeline/fit_decode.rs` (if proposal integration/plumbing changes are required)
- `crates/ringgrid/benches/hotpaths.rs` (benchmark coverage updates)

## Public API Impact

- [x] No API changes
- [ ] New public types (list them)
- [ ] Changed type signatures (list them)
- [ ] New config fields (list them, must have `Default`)
- [ ] New `Detector` methods (justify)

## Acceptance Criteria

- [ ] `proposal_1280x1024` benchmark median improves by >=10% versus PERF-001 baseline `42.404 ms` (target <= `38.164 ms`).
- [ ] `proposal_1920x1080` benchmark median improves by >=8% versus PERF-001 baseline `60.614 ms` (target <= `55.765 ms`).
- [ ] Representative `detect()` flamegraph shows reduced proposal hotspot share versus PERF-001 baseline (`61.11%`), or a written justification if wall-time shifts but share remains flat due whole-pipeline speedup.
- [ ] No new per-pixel/per-row/per-candidate allocations are introduced in proposal hot loops.
- [ ] `cargo test --workspace --all-features` passes.
- [ ] `cargo clippy --all-targets --all-features -- -D warnings` clean.
- [ ] Validation gate is complete with all required runs:
  - `./.venv/bin/python3 tools/run_synth_eval.py --n 10 --blur_px 3.0 --marker_diameter 32.0 --out_dir <...>`
  - `bash tools/run_reference_benchmark.sh`
  - `bash tools/run_distortion_benchmark.sh`

## Accuracy Constraints

- Center error target: mean regression <= +0.01 px versus PERF-001 comparison baseline.
- Decode success rate: no measurable drop beyond expected deterministic variance on comparison batch.
- Homography reprojection: no material regression (investigate if mean delta > +0.02 px).

## Performance Constraints

- Latency budget: focus on proposal-stage throughput reduction with explicit benchmark deltas.
- Allocation limits: do not introduce new dynamic allocations in gradient/voting/NMS inner loops.

## Python Tooling Changes

- [x] No Python changes needed
- [ ] New scoring metric in `score_detect.py`
- [ ] New visualization in `viz_detect_debug.py`
- [ ] New synthetic generation option in `gen_synth.py`
- [ ] Other: [describe]

## Notes

- PERF-001 baseline reference: `.ai/state/sessions/2026-02-16-PERF-001-baseline-report.md`
- If optimization requires changing proposal search strategy semantics, involve Algorithm Engineer with evidence-based handoff.
