# Task: PERF-003: Standardize Performance Validation Suite

- **Type:** perf
- **Priority:** P1
- **Requesting role:** Project Lead
- **Assigned workflow:** performance-optimization

## Problem Statement

PERF tasks now repeatedly require the same three validation gates (blur=3 synth eval, reference benchmark script, distortion benchmark script), but execution and reporting are still ad hoc across handoffs. We need a standardized validation suite and reporting format so every performance optimization is judged with consistent commands, comparable outputs, and explicit pass/fail gates.

## Affected Pipeline Stages

Which of the 13 stages are impacted? List by number and name.

1. [x] Proposal
2. [x] Outer Estimate
3. [x] Outer Fit
4. [x] Decode
5. [x] Inner Estimate
6. [x] Dedup
7. [x] Projective Center (1st pass)
8. [x] Global Filter
9. [x] H-guided Refine
10. [x] Projective Center (2nd pass)
11. [x] Completion
12. [x] Projective Center (3rd pass)
13. [x] Final H Refit

## Affected Modules

List file paths under `crates/ringgrid/src/`:

- `crates/ringgrid/src/` (no production Rust changes expected; task is validation/tooling/process-focused)

## Public API Impact

- [x] No API changes
- [ ] New public types (list them)
- [ ] Changed type signatures (list them)
- [ ] New config fields (list them, must have `Default`)
- [ ] New `Detector` methods (justify)

## Acceptance Criteria

- [x] A single documented validation runbook is produced in session artifacts with exact commands and required outputs for:
  - blur=3.0 synth eval (`n=10`)
  - `run_reference_benchmark.sh`
  - `run_distortion_benchmark.sh`
- [x] `templates/accuracy-report.md` is used to report all three gates with baseline/after-change deltas.
- [x] Validation handoff template content is standardized for PERF tasks (required metric table fields and artifact paths).
- [x] At least one dry-run validation report is produced from current PERF baseline/after data showing the standardized format is usable.
- [x] Backlog/workflow references are aligned so future PERF tasks explicitly point to this standardized suite.

## Accuracy Constraints

- Center error target: standardized report must flag `> +0.01 px` mean center error delta as failure/escalation.
- Decode success rate: standardized report must include precision/recall deltas for blur=3 run.
- Homography reprojection: standardized report must include self and vs-GT deltas and escalation guidance.

## Performance Constraints

- Latency budget: not a direct optimization task; enforce comparability of performance evidence across PERF tasks.
- Allocation limits: no constraints for this process task unless supporting instrumentation is added.

## Python Tooling Changes

- [ ] No Python changes needed
- [ ] New scoring metric in `score_detect.py`
- [ ] New visualization in `viz_detect_debug.py`
- [ ] New synthetic generation option in `gen_synth.py`
- [x] Other: standardize validation orchestration/reporting around benchmark shell scripts (`run_blur3_benchmark.sh`, `run_reference_benchmark.sh`, `run_distortion_benchmark.sh`)

## Notes

- Starting role for this task is Validation Engineer because scope is validation process/tooling standardization.
- Reuse prior PERF artifacts for dry-run examples:
  - `.ai/state/sessions/2026-02-16-PERF-004-performance-handoff.md`
  - `.ai/state/sessions/2026-02-16-PERF-005-performance-handoff.md`
